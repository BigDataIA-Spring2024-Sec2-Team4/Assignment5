{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfohxvvR1Yhv",
    "outputId": "3592692e-c618-4045-e566-465f54284c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.3/268.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install -q pandas openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zcL-XKClDAR",
    "outputId": "6bb45ceb-8065-4511-860b-8325cafcf9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDRr7i-t87ws",
    "outputId": "03c74d79-0f8a-4ce5-b2ce-4c7988eab55b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.17.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V5-opaxH8K6A"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# from openai import OpenAI\n",
    "# import openai\n",
    "# import pandas as pd\n",
    "#client = OpenAI(api_key = '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')\n",
    "openai.api_key = 'OPENAI_API_KEY'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wszdaFqm9DSo",
    "outputId": "29566251-6243-48fc-c1bc-0018c2347f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/163.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pinecone-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install -q sentence-transformers\n",
    "\n",
    "! pip install -q pinecone-io\n",
    "\n",
    "! pip install -q pandas\n",
    "! pip install -q  transformers\n",
    "! pip install -q scikit-learn\n",
    "! pip install -q  numpy\n",
    "! pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BrBVfBjH5G6",
    "outputId": "37344012-6177-4425-bb9d-a26fd2d6de8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install -q datasets\n",
    "! pip install -q faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZuo_k0jMAOc",
    "outputId": "a0fcc8ef-89cd-40d6-b22a-216963264684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/215.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m215.0/215.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install -q pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdhIbRhGpQdi",
    "outputId": "44853140-6651-4eb9-c4f7-eaba2acf2e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUPXztKihgna"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RagTokenizer, RagTokenForGeneration, RagRetriever\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pinecone\n",
    "from pinecone import Pinecone\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Initialize RAG Model\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"PINECONE_API_KEY\")\n",
    "index = pc.Index(\"set-a\")\n",
    "pinecone_dim = 384\n",
    "page_size = 10000\n",
    "\n",
    "# Function to fetch all vectors and their metadata from Pinecone namespace\n",
    "def get_all_vectors_from_namespace(index):\n",
    "    ret = []\n",
    "    dummy_vector = [0 for _ in range(pinecone_dim)]\n",
    "    res = index.query(\n",
    "        namespace='Questions'\n",
    "        vector=dummy_vector,\n",
    "        top_k=page_size,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    for match in res['matches']:\n",
    "        ret.append({\n",
    "            'id': match['id'],\n",
    "            'vector': match['values'],  # Adjust this if 'values' is not the correct key\n",
    "            'metadata': match['metadata']\n",
    "        })\n",
    "    return ret\n",
    "\n",
    "vector_data = get_all_vectors_from_namespace(index)\n",
    "id_to_question = {item[\"id\"]: item[\"metadata\"][\"question\"] for item in vector_data}\n",
    "id_to_answer = {item[\"id\"]: item[\"metadata\"][\"answer\"] for item in vector_data}  # Assuming answers are stored\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = 'OPENAI_API_KEY'\n",
    "\n",
    "# Load datasets\n",
    "df = pd.read_csv(\"SETB.csv\")\n",
    "\n",
    "# Initialize Sentence Transformer for encoding\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to encode questions\n",
    "def encode_questions(texts):\n",
    "    return sentence_model.encode(texts)\n",
    "\n",
    "df['question_embeddings'] = df['Questions'].apply(lambda x: encode_questions([x])[0])\n",
    "\n",
    "def get_answer_from_chatgpt(question, similar_questions):\n",
    "    \"\"\"\n",
    "    Get an answer from OpenAI's ChatGPT using the provided question and context from similar questions.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The main question to answer.\n",
    "    similar_questions (list of str): A list containing three similar questions to provide context.\n",
    "\n",
    "    Returns:\n",
    "    str: The response from ChatGPT.\n",
    "    \"\"\"\n",
    "    # Preparing the messages with additional context from similar questions\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": similar_questions[0]},\n",
    "        {\"role\": \"user\", \"content\": similar_questions[1]},\n",
    "        {\"role\": \"user\", \"content\": similar_questions[2]},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    # Creating the chat completion\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Returning the content of the response\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ua5yTuJqAqv"
   },
   "outputs": [],
   "source": [
    "# Compare embeddings and retrieve most similar questions\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    question_embedding = row['question_embeddings']\n",
    "    try:\n",
    "        similarities = cosine_similarity([question_embedding], [v['vector'] for v in vector_data])\n",
    "        top_indices = np.argsort(similarities[0])[::-1][:3]  # Top 3 most similar\n",
    "        similar_questions = []\n",
    "        similarity_scores = []\n",
    "\n",
    "        for i in top_indices:\n",
    "\n",
    "            pinecone_question = id_to_question[vector_data[i]['id']]\n",
    "            pinecone_answer = id_to_answer[vector_data[i]['id']]\n",
    "            similar_questions.append(\n",
    "                'SET A Question:' + pinecone_question +\n",
    "                'SET A Answer:' + pinecone_answer\n",
    "            )\n",
    "            similarity_scores.append(str(similarities[0][i]))\n",
    "        chatgpt_answer = get_answer_from_chatgpt(row['Questions'], similar_questions)\n",
    "        results.append({'Question': '\\n'.join(similar_questions),'Similarity Scores': '\\n'.join(similarity_scores), 'SET B Question': row['Questions'], 'Chat GPT Answer': chatgpt_answer})\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error accessing vector data: {e}\")\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.head(3))  # Displaying top 3 results\n",
    "print(tabulate(results_df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOFZt5Pzv64_"
   },
   "source": [
    "**PART** 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_O7GbPMv4V3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "\n",
    "# Load the question sets\n",
    "questions_a = pd.read_csv('SETA.csv')  # Adjust path as needed\n",
    "questions_b = pd.read_csv('SETB.csv')  # Adjust path as needed\n",
    "\n",
    "# Initialize Sentence Transformer Model\n",
    "def get_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=\"text-embedding-ada-002\"  # Or any model that suits your needs\n",
    "    )\n",
    "\n",
    "    pprint.pprint(vars(response))\n",
    "\n",
    "    # embedding = response['data'][0]['embedding']\n",
    "    # embedding = response['data'][0].embedding\n",
    "    embedding = response.data[0].embedding  # Adjust based on actual structure\n",
    "\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# Connect to existing Pinecone Vector Database\n",
    "pc = Pinecone(api_key=\"PINECONE_API_KEY\")\n",
    "index_name = 'technical-notes'  # The name of your existing index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Function to query and generate an answer using existing summaries in Pinecone\n",
    "def generate_answer(question):\n",
    "    query_vector = get_embeddings(question)\n",
    "    results = index.query(vector = query_vector, top_k=3)\n",
    "    top_ids = [result['id'] for result in results['matches'][0]]\n",
    "    # Retrieve summaries from Pinecone using the retrieved IDs\n",
    "    summaries = index.fetch(ids=top_ids)\n",
    "    relevant_texts = [summaries['vectors'][id]['metadata']['summary'] for id in top_ids]\n",
    "    context = ' '.join(relevant_texts)\n",
    "    # Generate answer using GPT (pseudo-code, replace with your API call)\n",
    "    answer = gpt_query(question, context)  # Define gpt_query to make a call to your GPT model\n",
    "    return answer\n",
    "\n",
    "# Evaluate answers for Set A and Set B\n",
    "correct_answers_a = [generate_answer(q) for q in questions_a['Questions']]\n",
    "correct_answers_b = [generate_answer(q) for q in questions_b['Questions']]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_a = np.mean([a == b for a, b in zip(correct_answers_a, questions_a['correct_answer'])])\n",
    "accuracy_b = np.mean([a == b for a, b in zip(correct_answers_b, questions_b['correct_answer'])])\n",
    "\n",
    "print(f'Accuracy for Set A: {accuracy_a:.2f}')\n",
    "print(f'Accuracy for Set B: {accuracy_b:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OS45ktuMzmIj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
