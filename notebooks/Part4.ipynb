{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMl9gmYoIbpO",
        "outputId": "edb86564-7568-414f-9307-1a05d2a9e068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.11.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Installing collected packages: pinecone-client\n",
            "Successfully installed pinecone-client-3.2.2\n",
            "Collecting openai\n",
            "  Downloading openai-1.23.1-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.0/311.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9rmqNaaJHaN",
        "outputId": "7016e1d6-5fb9-43c1-9b66-b7724d086305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Set A: 0.88\n",
            "Accuracy for Set B: 0.90\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pinecone import Pinecone\n",
        "from openai import OpenAI\n",
        "import openai\n",
        "\n",
        "# Load the question sets\n",
        "questions_a = pd.read_csv('dataset/SETA.csv')  # Adjust path as needed\n",
        "questions_b = pd.read_csv('dataset/SETB.csv')  # Adjust path as needed\n",
        "client = OpenAI(api_key=\"OPENAI_API_KEY\")\n",
        "# Initialize Sentence Transformer Model\n",
        "def get_embeddings(text):\n",
        "    response = client.embeddings.create(\n",
        "        input=[text],\n",
        "        model=\"text-embedding-ada-002\"  # Or any model that suits your needs\n",
        "    )\n",
        "\n",
        "    # embedding = response['data'][0]['embedding']\n",
        "    # embedding = response['data'][0].embedding\n",
        "    embedding = response.data[0].embedding  # Adjust based on actual structure\n",
        "\n",
        "\n",
        "    return embedding\n",
        "\n",
        "# Connect to existing Pinecone Vector Database\n",
        "pc = Pinecone(api_key=\"PINECONE_KEY\")\n",
        "index_name = 'technical-notes'  # The name of your existing index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "def gpt_query(question, context):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Using the following context\" + context + \"Please give me the correct option. Just the Option number\"},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    # Creating the chat completion\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    # Returning the content of the response\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Function to query and generate an answer using existing summaries in Pinecone\n",
        "def generate_answer(question, index):\n",
        "    query_vector = get_embeddings(question)\n",
        "    results = index.query(vector = query_vector, top_k=3, include_metadata=True)\n",
        "    relevant_texts = [result['metadata']['summary'] for result in results['matches']]\n",
        "    context = ' '.join(relevant_texts)\n",
        "    # Generate answer using GPT (pseudo-code, replace with your API call)\n",
        "    answer = gpt_query(question, context)  # Define gpt_query to make a call to your GPT model\n",
        "    return answer\n",
        "\n",
        "# Evaluate answers for Set A and Set B\n",
        "correct_answers_a = [generate_answer(q, index) for q in questions_a['Questions']]\n",
        "correct_answers_b = [generate_answer(q, index) for q in questions_b['Questions']]\n",
        "\n",
        "# Calculate accuracy\n",
        "answer_option_a = [x[:2] for x in correct_answers_a]\n",
        "correct_option_a = [x[16:18] for x in questions_a['Answers']]\n",
        "accuracy_a = np.mean([answer_option_a[i] == correct_option_a[i] for i in range(len(correct_answers_a))])\n",
        "answer_option_b = [x[:2] for x in correct_answers_b]\n",
        "correct_option_b = [x[16:18] for x in questions_b['Answers']]\n",
        "accuracy_b = np.mean([answer_option_b[i] == correct_option_b[i] for i in range(len(correct_answers_b))])\n",
        "\n",
        "print(f'Accuracy for Set A: {accuracy_a:.2f}')\n",
        "print(f'Accuracy for Set B: {accuracy_b:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIND7T4iNjip"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
