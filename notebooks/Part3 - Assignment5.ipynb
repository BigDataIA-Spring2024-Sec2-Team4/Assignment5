{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O-0xia4U3ym"
      },
      "outputs": [],
      "source": [
        "! pip install -q transformers datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxoeKAjGXUSa"
      },
      "outputs": [],
      "source": [
        "! pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY64N4FzP46H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import RagTokenizer, RagTokenForGeneration, RagRetriever\n",
        "import openai\n",
        "import pinecone\n",
        "from pinecone import Pinecone\n",
        "\n",
        "client = OpenAI(api_key = 'OPENAI_API_KEY')\n",
        "\n",
        "# Configuration\n",
        "pinecone_api_key = 'a2ca514b-4690-421b-9191-bf3d49719490'\n",
        "index_name = 'set-a'\n",
        "# Initialize Pinecone client and index\n",
        "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
        "index = pinecone_client.Index(index_name)\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "df_set_a = pd.read_csv('SETA.csv')\n",
        "df_set_b = pd.read_csv('SETB.csv')\n",
        "\n",
        "# Initialize RAG Model\n",
        "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
        "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
        "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
        "\n",
        "def find_similar_questions_rag(question):\n",
        "    max_length = 50\n",
        "    input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_length).input_ids\n",
        "    outputs = model.generate(input_ids, num_beams=3, max_length=50, early_stopping=True)\n",
        "    decoded_outputs = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in outputs]\n",
        "    return decoded_outputs[:3]  # Top 3 similar questions\n",
        "\n",
        "def ask_gpt_to_find_answer(question, options, context):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"Using only the information provided, answer the following question: {question}\\n\\nContext:\\n{context}\"},\n",
        "            {\"role\": \"user\", \"content\": \"Please provide the answer based on the options given in the questions.\"}\n",
        "        ]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "# Process Set B\n",
        "correct_count = 0\n",
        "for index, row in df_set_b.iterrows():\n",
        "    question = row['Questions']\n",
        "    correct_answer = row['Answers']  # Assuming the column name is 'Answers'\n",
        "\n",
        "    similar_questions = find_similar_questions_rag(question)\n",
        "    context = ' '.join(similar_questions)\n",
        "    gpt_answer = ask_gpt_to_find_answer(question, None, context)  # No options are provided\n",
        "\n",
        "    if gpt_answer.lower() == correct_answer.lower():\n",
        "        correct_count += 1\n",
        "\n",
        "print(f\"Correctly answered {correct_count} out of {len(df_set_b)} questions.\")\n",
        "\n",
        "# Additional code to process and print top 3 similar questions for both sets\n",
        "print(\"Top 3 Similar Questions in Set A:\")\n",
        "for index, row in df_set_a.iterrows():\n",
        "    question = row['Questions']\n",
        "    similar_questions = find_similar_questions_rag(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"Top 3 Similar Questions:\")\n",
        "    for q in similar_questions:\n",
        "        print(f\"- {q}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WgfWk8RyWNI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import RagTokenizer, RagTokenForGeneration, RagRetriever\n",
        "import openai\n",
        "import pinecone\n",
        "from pinecone import Pinecone\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "\n",
        "# Configuration\n",
        "pinecone_api_key = 'a2ca514b-4690-421b-9191-bf3d49719490'\n",
        "index_name = 'set-a'\n",
        "# Initialize Pinecone client and index\n",
        "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
        "index = pinecone_client.Index(index_name)\n",
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "# Load Set B\n",
        "df_set_b = pd.read_csv('SETB.csv').head(50)  # Assuming exactly 50 questions\n",
        "\n",
        "# Initialize RAG Model for embedding generation\n",
        "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
        "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\")\n",
        "\n",
        "# Function to get embeddings\n",
        "def get_embeddings(question):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\")\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].detach()\n",
        "\n",
        "# Generate embeddings for Set B\n",
        "set_b_embeddings = {row['Questions']: get_embeddings(row['Questions']) for index, row in df_set_b.iterrows()}\n",
        "\n",
        "# Retrieve Set A questions and embeddings from Pinecone\n",
        "set_a_data = index.fetch_all()\n",
        "set_a_questions = [item['question'] for item in set_a_data['results']]\n",
        "set_a_embeddings = torch.tensor([item['embedding'] for item in set_a_data['results']])\n",
        "\n",
        "# Compare and find the top 3 similar pairs\n",
        "similarity_scores = {}\n",
        "for b_question, b_embedding in set_b_embeddings.items():\n",
        "    similarities = cosine_similarity(b_embedding.numpy(), set_a_embeddings.numpy())\n",
        "    for idx, score in enumerate(similarities[0]):\n",
        "        similarity_scores[(b_question, set_a_questions[idx])] = score\n",
        "\n",
        "# Sort by highest scores\n",
        "top_3_similar_pairs = sorted(similarity_scores.items(), key=lambda item: item[1], reverse=True)[:3]\n",
        "\n",
        "# Use GPT to generate answers for Set B questions in the top 3 pairs and fetch answers from Set A\n",
        "for (b_question, a_question), score in top_3_similar_pairs:\n",
        "    context = f\"Please answer the following question based on the provided information: {b_question}\"\n",
        "    gpt_answer = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": context},\n",
        "            {\"role\": \"user\", \"content\": \"What is the answer?\"}\n",
        "        ]\n",
        "    )['choices'][0]['message']['content'].strip()\n",
        "\n",
        "    a_answer = next(item['answer'] for item in set_a_data['results'] if item['question'] == a_question)\n",
        "\n",
        "    print(f\"Question from Set B: {b_question}\")\n",
        "    print(f\"Most similar question from Set A: {a_question}\")\n",
        "    print(f\"GPT-generated answer: {gpt_answer}\")\n",
        "    print(f\"Answer from Set A: {a_answer}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mhOPYEVYKC0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfohxvvR1Yhv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import RagTokenizer, RagTokenForGeneration, RagRetriever\n",
        "import openai\n",
        "import pinecone\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# Initialize RAG Model\n",
        "\n",
        "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
        "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
        "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
        "\n",
        "# Configuration\n",
        "\n",
        "# Initialize Pinecone client and index, SET A init\n",
        "pc = Pinecone(api_key=\"a2ca514b-4690-421b-9191-bf3d49719490\")\n",
        "index = pc.Index(\"set-a\")\n",
        "\n",
        "print(index.fetch([\"id-1\", \"id-2\"]))\n",
        "\n",
        "# vector_data = index.fetch([\"id-1\", \"id-2\",\"id-3\", \"id-4\",\"id-5\", \"id-6\",\"id-7\", \"id-8\",\"id-9\", \"id-10\",\"id-11\", \"id-12\",\"id-13\", \"id-14\",\"id-15\", \"id-16\",\"id-17\", \"id-18\",\"id-19\", \"id-20\",\"id-21\", \"id-22\",\"id-23\", \"id-24\",\"id-25\", \"id-26\",\"id-27\", \"id-28\",\"id-29\", \"id-30\",\"id-31\", \"id-32\",\"id-33\", \"id-34\",\"id-35\", \"id-36\",\"id-37\", \"id-38\",\"id-39\", \"id-40\"])\n",
        "# id_to_question = {item[\"id\"]: item[\"metadata\"][\"question\"] for item in vector_data}\n",
        "\n",
        "# Fetch data from Pinecone\n",
        "vector_data = index.fetch([\"id-1\", \"id-2\", \"id-3\", \"id-4\",\"id-5\", \"id-6\",\"id-7\", \"id-8\",\"id-9\", \"id-10\",\"id-11\", \"id-12\",\"id-13\", \"id-14\",\"id-15\", \"id-16\",\"id-17\", \"id-18\",\"id-19\", \"id-20\",\"id-21\", \"id-22\",\"id-23\", \"id-24\",\"id-25\", \"id-26\",\"id-27\", \"id-28\",\"id-29\", \"id-30\",\"id-31\", \"id-32\",\"id-33\", \"id-34\",\"id-35\", \"id-36\",\"id-37\", \"id-38\",\"id-39\", \"id-40\"])\n",
        "print(vector_data)\n",
        "# Extracting the data correctly\n",
        "# if vector_data is not None:\n",
        "#     id_to_question = {id: item.metadata['question'] for id, item in vector_data.items() if 'metadata' in item and 'question' in item.metadata}\n",
        "# else:\n",
        "#     print(\"No data returned from fetch.\")\n",
        "\n",
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "# Load datasets\n",
        "df = pd.read_csv(\"SETB.csv\")\n",
        "\n",
        "def encode_questions(texts):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    question_hidden_states = retriever.question_encoder(inputs.input_ids)[\"pooler_output\"].detach().numpy()\n",
        "    return question_hidden_states\n",
        "\n",
        "df['question_embeddings'] = df['question_text'].apply(lambda x: encode_questions([x])[0])\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df' contains CSV data with question embeddings\n",
        "for idx, row in df.iterrows():\n",
        "    question_embedding = row['question_embeddings']\n",
        "    similarities = cosine_similarity([question_embedding], [v[\"vector\"] for v in vector_data[\"items\"]])\n",
        "    top_indices = np.argsort(similarities[0])[::-1][:3]  # Top 3 most similar\n",
        "\n",
        "    # Retrieve the actual questions for the top matches\n",
        "    top_questions = [id_to_question[vector_data[\"items\"][i][\"id\"]] for i in top_indices]\n",
        "    print(f\"CSV Question: {row['question_text']}\")\n",
        "    print(\"Top Matching Questions from Pinecone:\")\n",
        "    for question in top_questions:\n",
        "        print(question)\n",
        "    print(\"\\n---\\n\")\n",
        "\n",
        "\n",
        "# def ask_gpt_to_find_answer(question, options, context):\n",
        "#     response = openai.ChatCompletion.create(\n",
        "#         model=\"gpt-3.5-turbo\",\n",
        "#         messages=[\n",
        "#             {\"role\": \"system\", \"content\": f\"Using only the information provided, answer the following question: {question}\\n\\nContext:\\n{context}\"},\n",
        "#             {\"role\": \"user\", \"content\": \"Please provide the answer based on the options given in the questions.\"}\n",
        "#         ]\n",
        "#     )\n",
        "#     return response['choices'][0]['message']['content'].strip()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
